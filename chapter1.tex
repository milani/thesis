\chapter{پیش‌زمینه}\markboth{پیش‌زمینه}{پیش‌زمینه}\label{chap:prerequisites}
\clearpage
\section{نظریه گراف}
در این بخش، نظریه گراف و نحوه نمادگذاری‌ را مرور خواهیم کرد. نمادگذاری ارائه شده در این بخش برای پیگیری بخش‌ها و فصول بعدی مورد نیاز است.

\subsection{گراف، زیرگراف و یکریختی}
یک \خمیده{گراف}، یک دوتایی \گراف{G} است که در آن
$V = \{v_{1},v_{2},...,v_{n}\}$
یک مجموعه مرتب شامل $n$ گره‌ یا رأس است و
$E \subseteq V\times V$
مجموعه یال‌هاست. اندازه گراف برابر اندازه مجموعه \V تعریف می‌شود که در اینجا $n$ است.

گراف \گراف{G}، یک گراف \خمیده{بدون جهت} است اگر برای هر دو رأس
$v_{i},v_{j} \in V$
، از اینکه
$\edge{v}{i}{j}{E}$
بتوان نتیجه گرفت که
$\edge{v}{j}{i}{E}$
؛ در غیر این صورت گراف \خمیده{جهت‌دار} است.

به هر یال به صورت \یال{i}{i}
یک \خمیده{دور} گفته می‌شود. بطور کلی بین دو رأس \Vi و \Vj در یک گراف، می‌تواند بیش از یک یال وجود داشته باشد. یک \خمیده{گراف ساده}، گرافی بدون دور است که بین هر دو رأس آن حداکثر یک یال وجود داشته باشد.

در این پایان‌نامه منظور از گراف، گراف ساده بدون جهت است؛ در غیر این صورت عبارت دقیق قید خواهد شد.

یک گراف ساده را می‌توان با \خمیده{ماتریس مجاورت} \A به اندازه $n\times n$نمایش داد. درایه $(i,j)$ از ماتریس \A برابر 1 است اگر یال \یال{i}{j} وجود داشته باشد. در غیر این صورت این درایه برابر صفر است. بدیهی است که ماتریس مجاورت یک گراف بدون جهت، متقارن است.

رئوس و/یا یال‌های یک گراف می‌توانند برچسب داشته باشند. یک گراف برچسب‌دار را با سه‌تایی $(V,E,l)$ نشان می‌دهیم که در آن $l: X \mapsto \Sigma$ تابعی است که یک برچسب از الفبای $\Sigma$ را به هر عضو مجموعه \X نسبت می‌دهد که \X می‌تواند بسته به اینکه رئوس یا یال‌ها و یا هر دو برچسب‌گذاری شده باشند به ترتیب برابر \V یا \E و یا $V\cup E$ باشد.

دو گراف \گراف{G} و
$\graph{G^{\prime}}{V^{\prime}}{E^{\prime}}$
\خمیده{یکریخت} (با نماد $G^{\prime} \simeq G$) هستند اگر تابع نگاشت دوسویی $f: V \rightarrow V^{\prime}$ (تابع یکریختی) وجود داشته باشد به طوری که
$(v_{i},v_{j}) \in E$
اگر و تنها اگر
$(f(v_{i}),f(v_{j})) \in E^{\prime}$
. اگر $G = G^{\prime}$ باشد آنگاه به $f$ تابع \خمیده{خودریختی} می‌گویند. برای دو گراف برچسب‌دار $G(V,E,l)$ و $G^{\prime} = (V^{\prime},E^{\prime},l^{\prime})$ تابع یکریختی (همچنین خودریختی) باید رابطه 
$l(v_{i}) = l^{\prime}(f(v_{i}))$
 را به ازای هر $v_{i} \in V$ نیز ارضاء کند.
 
به یک تابع با آرگومان از نوع گراف، ناوردای گرافی
\پانوشت{\متن‌لاتین{graph invariant}} گفته می‌شود اگر به دو گراف یکریخت مقدار یکسانی نسبت دهد. مثلاً توابع تعداد رئوس و تعداد یال‌ها توابع ناوردای گرافی می‌باشند.

برای دو گراف \گراف{G} و $\graph{G^{\prime}}{V^{\prime}}{E^{\prime}}$، می‌گوییم $G^{\prime}$ یک \خمیده{زیرگراف} از $G$ است (با نماد $G^\prime \subseteq G$) اگر $V^\prime \subseteq V$ و $E^\prime \subseteq E$. اگر $G^\prime \subseteq G$ و $E^\prime$ شامل تمام یال‌های $(u,v) \in E$ باشد به طوری که $u,v \in V^\prime$، آنگاه می‌گوییم $G^\prime$ یک \خمیده{زیرگراف القایی} $G$ است و آن را با نماد $G^\prime \sqsubseteq G$ نشان می‌دهیم.

\subsection{همسایگی و درجه}
دو رأس \Vi و \Vj از گراف $G$ \خمیده{مجاور}\پانوشت{adjacent} و یا \خمیده{همسایه}\پانوشت{neighbour} هستند اگر $(v_i,v_j) \in E$. برای رأس $v$ همسایگی $\mN(v)$، مجموعه‌ی رئوسی است که در مجاورت آن قرار دارند؛ به عبارتی
$\mN(v) = \{v_i | (v,v_i) \in E\}$.
به تمام یال‌ها به فرم $(v,v_i) \in E$ \خمیده{یال‌های حادث}\پانوشت{incident} رأس $v$ گفته می‌شود.

درجه یک رأس مثل $v$ که با نماد $d(v)$ نشان داده می‌شود، تعداد یال‌های حادث با آن رأس است. برای گراف‌های ساده بدون جهت عدد اصلی\پانوشت{cardinality} مجموعه $\mN(v)$ با $d(v)$ برابر است.

\subsection{گشت، مسیر، دور، زیردرخت، الگوی زیردرختی}
یک \خمیده{گشت} در گراف، دنباله‌ای از رئوس است که دو رأس متوالی توسط یک یال به هم متصل شده‌باشند. یک \خمیده{مسیر} گشتی است که تمام رئوس آن غیرتکراری باشند. به یک مسیر بسته \خمیده{دور} گفته می‌شود. گراف G \خمیده{همبند} است اگر بین هر دو رأس آن یک مسیر وجود داشته باشد. \خمیده{فاصله} بین دو رأس $v$ و $v^\prime$ در $G$ طول کوتاه‌ترین مسیر بین دو رأس $v$ و $v^\prime$ در $G$ است و اگر چنین مسیری وجود نداشته باشد، فاصله را برابر $\infty$ در نظر می‌گیریم.

% به زیرگراف همبند $G^\prime = (V^\prime,E\prime)$ از گراف \گراف{G} یک \خمیده{مؤلفه همبندی} گفته می‌شود
 
یک زیردرخت (ریشه‌دار)، زیرگرافی بدون دور از یک گراف است که یک رأس از آن به عنوان ریشه در نظر گرفته می‌شود. ارتفاع این زیردرخت برابر است با طول طولانی‌ترین مسیر بین ریشه و سایر رئوس. همانطور که مفهوم گشت، توسعه‌ای از مفهوم مسیر است که در آن اجازه تکرار رئوس را داریم، مفهوم زیردرخت را می‌توان به مفهومی به اسم \خمیده{الگوی زیردرختی}\پانوشت{\متن‌لاتین{subtree pattern}} توسعه داد که در آن رئوس اجازه تکرار دارند (نام دیگری برای درخت-گشت‌\جستار{Bach_2008}.) رئوس تکراری به عنوان رئوس مجزا در این درخت در نظر گرفته می‌شوند به طوری که مفهوم درخت حفظ شود، یعنی در دور نداشته باشیم (شکل \ارجا{fig:subtree-pattern}).

\section{مروری بر مقایسه گراف‌ها}\label{sec:graph_comparison}
مسئله اصلی مورد مطالعه در این پایان‌نامه، مقایسه گراف‌هاست. این مسئله در قالب فرمال به صورت زیر تعریف می‌شود.

\begin{definition}[مسئله مقایسه گراف‌ها]
اگر $\mG$ مجموعه‌ای از تمام گراف‌های ممکن باشد، مسئله مقایسه گراف‌ها، تعریف تابعی به شکل
\begin{equation*}
k:\mG \times \mG \mapsto \R,
\end{equation*}
است بطوری که $k(G,G^\prime)$ برای $G,G^\prime \in \mG$ نمایانگر شباهت $G$ و $G^\prime$ باشد.
\end{definition}
مقالات بسیاری در مبحث مقایسه دو گراف منتشر شده‌است که بطور کلی به دو دسته قابل تقسیم‌اند.

\subsection{روش‌های مبتنی بر یکریختی}
در این روش‌ها، از مفهوم یکریختی و یا مفاهیم مشابه نظیر زیرگراف یکریخت و بزرگترین زیرگراف مشترک استفاده می‌شود. بطور طبیعی ساده‌ترین راه اندازه‌گیری شباهت دو گراف این است که ببینیم آیا ساختار آن‌دو یکسان است یا خیر؛ که این مفهوم یکریختی است. این تعریف، سبب ایجاد تابعی می‌شود که دو مقدار دارد: اگر دو گراف یکریخت باشند، یک است و در غیر این صورت صفر است. اگرچه ایده‌ی این روش بسیار ملموس است، اما هیچ الگوریتم بهینه‌ای برای اینکار وجود ندارد. مسئله بررسی یکریختی گراف، NP‌ است اما اینکه مسئله \متن‌لاتین{NP-complete} است یا الگوریتم حل در زمان خطی برای آن وجود دارد هنوز اثبات نشده‌است\جستار{Garey_1979}.

بررسی زیرگراف یکریخت همانند بررسی یکریختی گراف‌هاست، اما برای دو گراف با اندازه‌های مختلف تعریف می‌شود. برخلاف مسئله یکریختی، اثبات شده‌است که مسئله زیرگراف یکریخت جزء مسائل \متن‌لاتین{NP-complete} است\جستار{Garey_1979}.

یک معیار شباهت دیگر که محدودیت کمتری دارد، بر اساس اندازه بزرگترین زیرگراف مشترک بین دو گراف تعریف می‌شود. ولی بازهم مسئله پیدا کردن این زیرگراف \متن‌لاتین{NP-hard} است\جستار{Garey_1979}.

\subsection{روش‌های مبتنی بر فاصله ویرایش}
روش‌های مبتنی بر یکریختی، علاوه بر اینکه از لحاظ محاسباتی تقریباً غیرقابل استفاده هستند، گستره محدودی نیز دارند؛ از این لحاظ که دو گراف تحت بررسی یا باید دقیقاً یکسان باشند و یا زیرگراف‌های بزرگ مشترکی داشته باشند تا شبیه شمرده شوند. معیارهای انعطاف‌پذیرتری بر مبنای تطابق غیردقیق دو گراف وجود دارد. معیارهای مبتنی بر فاصله ویرایش در این دسته قرار دارند\جستار{Sanfeliu_1983}\جستار{Bunke_1983}\جستار{Messmer_1998}\جستار{Neuhaus_2005}. بر اساس این روش، دو گراف $G$ و $G^\prime$ شبیه هستند اگر یکی از آن‌ها را بتوان توسط تعداد کمی تغییر ساده مثل حذف و اضافه کردن رأس یا یال و یا تغییر برچسب‌گذاری (برای گراف‌های برچسب‌دار)، به دیگری تبدیل کرد. برای هر نوع تغییر هزینه‌‌ای متفاوت در نظر گرفته می‌شود و مقدار شباهت، برابر کمترین هزینه برای تبدیل $G$ به $G^\prime$ خواهد بود. متأسفانه پیدا کردن هزینه بهینه مسئله ساده‌ای نیست.

بطور خلاصه روش‌های مبتنی بر فاصله ویرایش، اگرچه معیارهای خوبی برای اندازه‌گیری شباهت ساختار گراف‌ها، و حتی ند‌ها و یال‌ها هستند، ولی در مراحل میانی اجرای الگوریتم، مسائل \متن‌لاتین{NP-complete} بوجود می‌آیند که در زمان خوبی قابل حل نیستند.

\section{مروری بر روش‌های نمایش گراف}\label{sec:graph_representation}
مسئله نمایش گراف به شکل زیر مطرح می‌شود.
\begin{definition}[مسئله نمایش گراف]
اگر $\mG$ مجموعه‌ای از تمام گراف‌های ممکن باشد، مسئله نمایش گراف‌، تعریف تابعی به شکل
\begin{equation*}
\phi:\mG \mapsto \R^p, p \in \N
\end{equation*}
است بطوری که برای هر $G \in \mG$،
 $\phi(G)$
  نمایانگر ساختار $G$ باشد.
\end{definition}

نمایش گراف و شباهت گراف دو مسئله نزدیک و مرتبط هستند: واضح است که اگر راه بهینه و کاملی برای خلاصه کردن ساختار یک گراف در قالب یک بردار ویژگی\پانوشت{\متن‌لاتین{feature vector}} وجود داشت، مقایسه گراف‌ها هم کار مشکلی نبود. در این بخش سه دسته مهم از روش‌های نمایش گراف را مختصر مرور می‌کنیم. همانطور که مشخص است هدف از این روش‌ها محاسبه برداری است که نمایانگر ساختار گراف باشد. هرکدام از این روش‌ها را می‌توان مستقیماً در تعریف یک تابع شباهت برای گراف‌ها بکار برد.

\subsection{واصف ساختاری}
خلاصه کردن ساختار یک گراف در قالب یک بردار برای اولین بار در شیمی محاسباتی صورت گرفت‌\جستار{Gasteiger_Engel_2003}. همانطور که در مقدمه ذکر شد، یک راه استاندارد برای نمایش مولکول، مدل کردن آن توسط گراف است که رئوس گراف، اتم‌ها و یال‌های گراف نمایانگر پیوندها هستند. یک \خمیده{واصف ساختاری}\پانوشت{\متن‌لاتین{topological descriptor}}(یا شاخص ساختاری\پانوشت{\متن‌لاتین{topological index}}) یک عدد یا بردار عددی است که ساختار گراف را توصیف می‌کند. این واصف‌های برداری، معمولاً  ناوردای گرافی هستند و در مسائل شیمی محاسباتی بجای ساختار مولکول‌ها بکار می‌روند.  از سال ۱۹۵۰، تعداد زیادی واصف ساختاری برای گراف‌ها تعریف شد (برای مطالعه کامل این واصف‌ها مراجعه شود به \جستار{Todeschini_2008}).

از مهمترین واصف‌های ساختاری، می‌توان به شاخص‌های وینر\پانوشت{Wiener}\جستار{Wiener_1947}، مرگان\پانوشت{Morgan}\جستار{Morgan_1965} و زاگرب\پانوشت{Zagreb}\جستار{Gutman_1972} اشاره کرد که در ادامه به توضیح داده خواهند شد. شاخص وینر، مجموع طول تمام کوتاهترین مسیرهای یک گراف است.

\begin{definition}[شاخص وینر]
برای گراف \گراف{G}، شاخص وینر (عدد وینر) برابر است با
\begin{equation*}
W(G) = \sum_{v_i\in V}\sum_{v_j\in V} D_{ij}
\end{equation*}
که $D_{ij}$ طول کوتاهترین مسیر بین رئوس \Vi و \Vj است.
\end{definition}

شاخص مرگان، توسط رابطه بازگشتی زیر تعریف می‌شود.
\begin{definition}[شاخص مرگان]
برای گراف \گراف{G}، شاخص مرگان درجه $k$ برای هر رأس $v \in V$ برابر است با
\begin{equation*}
M_k(G,v) = 
\left\{
	\begin{array}{ll}
		\1  & \mbox{if } k = \0, \\
		{\sum_{v^\prime\in \mN(v)}M_{k-\1}(v^\prime)} & otherwise.
	\end{array}
\right.
\end{equation*}
در واقع، شاخص مرگان درجه $k$ برای رأس $v$ برابر است با تعداد گشت‌هایی به طول $k$ با شروع از رأس $v$ در گراف $G$.
\end{definition}

\begin{definition}[شاخص زاگرب]
برای گراف \گراف{G}، شاخص‌های اول و دوم زاگرب به ترتیب به صورت
\begin{equation*}
Z_\1(G) = \sum_{v\in V}(d(v))^2
\end{equation*}
و
\begin{equation*}
Z_\2(G) = \sum_{(v,v^\prime)\in E}d(v)d(v^\prime)
\end{equation*}
تعریف می‌شوند که $d(v)$ درجه رأس $v$ است.
\end{definition}

هر سه این تعاریف، ناوردای گرافی هستند: یعنی اگر دو گراف $G$ و $G^\prime$ یکریخت باشند، مقدار واصف ساختاری آن‌ها تحت تعاریف بالا یکسان خواهد بود. ولی عکس این حالت صادق نیست. واضح است که تعریف واصف گرافی که بتواند حالت عکس را نیز تأمین کند، معادل حل مسئله یکریختی است که برای آن الگوریتم خطی پیدا نشده‌است.  باید توجه داشت که ممکن است یکی از واصف‌های گرافی در یک مسئله، پاسخ‌های بهتری بدهد ولی در مسئله دیگر واصف دیگری بهترین پاسخ را داشته باشد. انتخاب بهترین واصف گرافی از بین تعداد زیادی از آن‌ها برای یک مسئله خاص، بسیار سخت است.
\subsection{استخراج الگو‌های متواتر}
استخراج الگوهای متواتر\پانوشت{\متن‌لاتین{frequent pattern mining}} یکی زیرشاخه‌های داده‌کاوی است. اگر زیرگراف‌ها را به عنوان الگو در نظر بگیریم، آنگاه استخراج زیرگراف‌های متواتر، روشی برای یافتن زیرگراف‌هایی است که در یک مجموعه گراف، بیشترین تکرار را دارند.

نمایش گراف بوسیله استخراج زیرگراف‌های متواتر، به این شکل انجام می‌شود: برای هر $G \in \mG$، و برای هر زیرگراف انتخابی $g_i$، تعداد تکرار آن زیرگراف در $G$ ثبت می‌شود. در پایان برای هر $G$ برداری به شکل
\begin{equation*}
(c_\1(G),...,c_p(G))
\end{equation*}
خواهیم داشت که در آن $c_i(G)$ برابر تعداد تکرار زیرگراف $g_i$ در $G$ است و $p$ تعداد این زیرگراف‌هاست.

بسیاری از الگوریتم‌های استخراج الگوهای متواتر بر مبنای خصوصیتی به نام \خمیده{انتاج}\پانوشت{Apriori}\جستار{Agrawal_1994} عمل می‌کنند. این مفهوم بیان می‌کند که اگر یک مجموعه $S$ در گردایه‌ای از مجموعه‌ها، متواتر باشد، آنگاه هر زیرمجموعه از $S$ هم متواتر خواهد بود. این مشاهده به سادگی قابل تعمیم به گراف‌هاست: اگر گراف $H$ در مجموعه‌ای از گراف‌ها مکرراً دیده شود، آنگاه هر زیرگرافی از $H$ هم حداقل به اندازه $H$ دیده‌خواهد شد، لذا این زیرگراف‌ها نیز متواتر خواهند بود. الگوریتم AGM \جستار{Inokuchi_2000} یک مثال خوب از الگوریتم‌های مبتنی بر انتاج برای استخراج زیرگراف‌های متواتر است.

روش‌های افزایش الگو\پانوشت{pattern-growth}\جستار{Han_2000}، دیگر گروه مهم در الگوریتم‌های استخراج زیرگراف‌های متواتر هستند. این الگوریتم‌ها، کار را با الگوهای کوچک آغاز می‌کنند و در مراحل بعد، آنها را گسترش می‌دهند تا حدی که الگوهای بدست آمده دیگر متواتر نباشند. مهمترین مثال‌ها از این خانواده، الگوریتم gspan \جستار{Yan_2002} و Gaston \جستار{Nijssen_2005} هستند. در gspan، یک ترتیب الفبایی برای گراف‌های برچسب‌دار بر حسب الگوریتم جستجوی عمق-اول (DFS) تعریف شده و هر گراف به یک برچسب متعارف یکتا (با نام \خمیده{کوتاهترین کد DFS}) نگاشت می‌شود. فضای جستجو در gspan به صورت درختی از کدهای DFS در نظر گرفته می‌شود که هر برگ در این درخت توسط روشی برای گسترش الگوها (با نام \خمیده{راست‌ترین بسط}\پانوشت{\متن‌لاتین{rightmost extension}}) بدست می‌آید. بدین طریق، gspan در حین جستجو برای زیرگراف‌های متواتر، از تست دوباره یک الگو و یا گسترش یک الگو که قبلاً دیده‌است جلوگیری می‌کند.

روش‌های جستجوی زیرگراف‌های متواتر، در مسائل یادگیری روی گراف‌های کوچک خوب عمل می‌کنند. مشکل بزرگ این روش‌ها این است که فضای جستجو در رابطه با اندازه گراف، رشد نمایی دارد و با وجود الگوریتم‌های هوشمندانه (همچون استفاده از روش‌های شاخه و حد\پانوشت{\متن‌لاتین{branch and bound}})، در بدترین حالت باید تمام زیرگراف‌های یک گراف را بشمارند که اینکار برای گراف‌های بزرگتر از بیست رأس عملاً غیرقابل انجام است.

\section{مروری بر گراف کرنل‌ها}
همانطور که در بخش‌های \ارجا{sec:graph_comparison} و \ارجا{sec:graph_representation} دیدیم، روش‌های سنتی مقایسه و نمایش دو گراف از مشکلاتی نظیر زمان اجرای نمایی و عدم توان در خلاصه‌سازی بهینه ساختار گراف رنج می‌برند و یا پارامتربندی و استفاده از آن‌ها در کاربردهای همه منظوره، دشوار است. گراف کرنل‌ها که اخیراً به عنوان ابزار یادگیری روی داده‌های ساختاری، برای خود جایی باز کرده‌اند، سعی در رفع این این مشکلات دارند. این ابزارها می‌توانند همزمان به خلاصه‌سازی و مقایسه گراف‌ها بپردازند، ولی با محدود کردن خود به زیرساختارهایی از گراف که اجازه محاسبه مقدار کرنل در زمان خطی را می‌دهد، از بروز مشکلات پیشین جلوگیری می‌کنند. گراف کرنل‌ها، فاصله بین داده‌های گرافی و طیف وسیعی از الگوریتم‌های یادگیری ماشین با نام \خمیده{روش‌های کرنل}\جستار{Scholkopf_2002} را پر می‌کنند. الگوریتم‌های \متن‌لاتین{kernel PCA}، \متن‌لاتین{SVM} از این دسته‌اند.

قبل از مرور گراف کرنل‌ها، توابع کرنل در یادگیری ماشین را مختصراً توضیح می‌دهیم. برای شرح جامع این روش‌ها به کتاب «یادگیری به وسیله کرنل‌ها»\جستار{Scholkopf_2002} مراجعه شود.

\subsection{کرنل‌ها در یادگیری ماشین}
بطور ساده، کرنل (مثبت معین) تابعی است که شباهت دو شیء را می‌سنجد. در این بخش ابتدا به معرفی کرنل می‌پردازیم و سپس یک مثال از کاربرد تابع کرنل را در قالب رده‌بندی دودویی\پانوشت{\متن‌لاتین{binary classification}} خواهیم دید.

\subsubsection{کرنل‌های مثبت معین}
در زیر تعریف کرنل‌های نیمه معین مثبت که از کتاب \جستار{Scholkopf_2004} آورده شده را می‌آوریم.
\begin{definition}[کرنل‌ نیمه معین مثبت]
اگر $\mX$ یک مجموعه ناتهی باشد، تابع $k: \mX \times \mX \mapsto \R$ یک تابع کرنل نیمه معین مثبت است اگر و تنها اگر متقارن باشد، یعنی برای هر $x,x^\prime \in \mX$ داشته باشیم
$k(x,x^\prime) = k(x^\prime,x)$، و نیمه معین مثبت باشد، 
یعنی برای هر $N \in \N$، هر انتخاب از اشیاء $x_\1,...,x_N \in \mX$ و هر انتخاب از اعداد حقیقی $c_\1,...,c_N \in \R$ داشته باشیم:

\begin{equation*}
\sum_{i=\1}^{N}\sum_{j=\1}^{N}c_ic_jk(x_i,x_j) \geq 0
\end{equation*}
\end{definition}

در یادگیری ماشین، به کرنل‌های نیمه معین مثبت معمولاً کرنل‌های مثبت معین یا خلاصه‌تر، کرنل گفته می‌شود. در ادامه این پایان‌نامه از واژه کرنل برای اشاره به این تعریف استفاده می‌کنیم.

چرا کرنل‌ها در یادگیری ماشین مورد توجه قرار گرفته‌اند؟ پاسخ این است که این توابع، امکان محاسبه ضرب‌داخلی در فضای با بعد بالا را در زمان بهینه فراهم می‌آورند که اینکار برای بسیاری از تکنیک‌های یادگیری ماشین ضروری است. شرح این پاسخ در ادامه آورده شده است.

خصوصیت اصلی کرنل‌ها که توجیه کننده ارتباط آن‌ها با ضرب داخلی است، این است که: برای هر کرنل نیمه معین مثبت $k: \mX \times \mX \mapsto \R$ یک فضای هیلبرت  بازتولیدِ کرنل \پانوشت{\متن‌لاتین{reproducing kernel Hilbert space}}
(RKHS) $\mH$ و یک تابع نگاشت $\phi: \mX \mapsto \mH$ وجود دارد بطوری که
به ازای هر $x,x^\prime \in \mX$،
$k(x,x^\prime)$
 برابر ضرب داخلی 
 $\phi(x)$ 
 و $\phi(x^\prime)$ است:
\begin{equation*}
k(x,x^\prime) = \langle\phi(x),\phi(x^\prime)\rangle.
\end{equation*}
عکس این خاصیت هم درست است: اگر $\mX$ یک مجموعه ناتهی باشد، $\mH$ یک فضای هیلبرت بازتولید کرنل باشد و $\phi: \mX \mapsto \mH$ تابع نگاشت باشد، آنگاه هر تابع $k: \mX\times \mX \mapsto \R$ که به وسیله $k(x,x^\prime) = \langle\phi(x),\phi(x^\prime)\rangle$ مشخص شده باشد، یک تابع کرنل نیمه معین مثبت است. در یادگیری ماشین، به $\mH$ فضای ویژگی\پانوشت{\متن‌لاتین{feature space}}،
به $\phi$ تابع نگاشت ویژگی\پانوشت{\متن‌لاتین{feature map}} و
به $\phi(x)$ نمایش ویژگی
\پانوشت{\متن‌لاتین{feature representation}}
$x$ می‌گویند.

در حقیقت، تعریف تابع نگاشت ویژگی، راه مستقیم تعریف یک کرنل است. به عنوان مثال فرض کنید که به ازای یک $p$، 
$\mX = \R^p$، $\mX = \mH$ و $\phi$ را برابر تابع همانی در نظر بگیریم، آنگاه تابع
\begin{equation*}
k(x,x^\prime) = \langle\phi(x),\phi(x^\prime)\rangle = \langle{x,x^\prime}\rangle
\end{equation*}
که یک ضرب داخلی ساده است، کرنل معتبری است.

این مثال در واقع راهی را برای طراحی هر تابع کرنل دلخواه، پیش رو گذاشت:
\begin{enumerate}
\فقره $\mH$ را انتخاب کنید،
\فقره تابع نگاشت $\phi$ را انتخاب کنید،
\فقره قرار دهید: $k(x,x^\prime) = \langle\phi(x),\phi(x^\prime)\rangle $.
\end{enumerate}

هرچند این روند برای تعریف کرنل، کاملاً درست است، ولی هیچ کمکی به محاسبه $\langle\phi(x),\phi(x^\prime)\rangle$ نمی‌کند: برای محاسبه $k(x,x^\prime)$، هنوز لازم است که ضرب و جمع در فضای $\mH$ صورت پذیرد که زمان اجرای آن متناسب با بُعد فضای $\mH$ خواهد بود. اما نکته جالب اینجاست که برای بعضی از کرنل‌ها، می‌توان بدون محاسبه در فضای $\mH$، مقدار $k(x,x^\prime)$ را بدست آورد. این ویژگی است که موجب اقبال روش‌های مبتنی بر کرنل در یادگیری ماشین شده‌است. به کمک این ویژگی، می‌توان نمایش داده در فضای $\mH$ را با بُعد بسیار بالا و حتی بی‌نهایت در نظر گرفت. برای روشن شدن موضوع، دو کرنل معروف \خمیده{چندجمله‌ای}\پانوشت{polynomial} و \خمیده{تابع شعاع محور گاوسی}(RBF)\پانوشت{\متن‌لاتین{gaussian radial basis function}} را در فضای $\R^p\times \R^p$ بررسی می‌کنیم.

\paragraph{کرنل چند جمله‌ای}
این کرنل به وسیله تابع زیر تعریف می‌شود:
\begin{equation*}
k(x,x^\prime) = (\langle{x,x^\prime}\rangle + c)^d
\end{equation*}
که در آن $d \in N$ و $c \geq \0 $ پارامتر هستند. می‌توان نشان داد که بُعد فضای ویژگی برای $k$ برابر تعداد تک جمله‌ای ها با درجه $d$ و یا کمتر در $p$ متغیر است، که برابر است با $p+d\choose d$. فرض کنید که $ p = 100 $ و $ d = 4 $: محاسبه $k(x,x^\prime)$ در ۲۰۲ عمل (۱۰۲ ضرب و ۱۰۰ جمع) انجام خواهد شد، درحالی که محاسبه مقدار تابع توسط نمایش ویژگی $\phi(x)$ و $ \phi(x^\prime) $، 
$2{104\choose 4} - 1 = 2\times 4598126−1 = 9196251$ عمل جمع و ضرب نیاز خواهد داشت. بنابراین کرنل چندجمله‌ای این امکان را فراهم می‌آورد که ضرب‌داخلی در فضای 
$\R^{4598126}$  را بجای حدود ۹ میلیون عمل، طی ۲۰۲ عمل ضرب و جمع انجام دهیم.
اینکار برای درجات بالاتر $d$ و مقادیر بزرگتر $p$ بهبود چشمگیری در زمان اجرای محاسبه است.

\paragraph{کرنل شعاع محور گاوسی}
 این کرنل به شکل 
\begin{equation}\label{eq:gaussian-kernel}
k(x,x^\prime) = exp(-\dfrac{\|x - x^\prime\|^2}{2\sigma^2})
\end{equation}
تعریف می‌شود که در آن $\sigma > 0$. ویژگی این کرنل آن است که فضای \متن‌لاتین{RKHS} متناظر با آن بی‌نهایت-بُعدی است. نمایشِ داده در این فضا غیر ممکن است، اما با کمک تابع کرنل می‌توان مقدار ضرب‌داخلی در این فضا را بدست آورد.

چطور بدون انتخاب صریح فضای $\mH$ و تابع $\phi$ یک کرنل نیمه معین مثبت تعریف کنیم؟ یک راه ساده، استفاده از خاصیت بسته بودن مجموعه کرنل‌های نیمه معین مثبت نسبت به بعضی از عمل‌هاست که به این وسیله می‌توان کرنل‌های جدیدی تعریف کرد\جستار{Scholkopf_2002}. در زیر به بعضی از این خواص اشاره می‌کنیم:

\begin{itemize}
\فقره اگر $k_1$ و $k_2$ کرنل و 
$\alpha_1,\alpha_2 \geq 0$ باشند
 آنگاه $\alpha_1k_1 + \alpha_2k_2$ یک کرنل است.
\فقره اگر $k_1,k_2,...$ کرنل باشند و 
$k(x,x^\prime) = \lim_{n \to \infty}k_n(x,x^\prime)$
برای تمام $x$ و $x^\prime$ وجود داشته باشد، آنگاه $k$ کرنل است.
\فقره اگر $k_1$ و $k_2$ کرنل باشند آنگاه $k_1k_2$ که به صورت 
$k_1k_2(x,x^\prime) = k_1(x,x^\prime)k_2(x,x^\prime)$ تعریف می‌شود، هم کرنل است .
\end{itemize}

کرنل‌ها علاوه بر اینکه امکان محاسبه ضرب داخلی در فضای با بعد بالا را فراهم می‌آورند، این قدرت را دارند که از داده‌های غیربرداری هم استفاده کنند. اگر برای هر شیء دلخواه، تابع متقارنی به عنوان مقیاس شباهت آن اشیاء تعریف کنیم، و اثبات کنیم که این تابع، نیمه معین مثبت است آنگاه بطور خودکار نمایشِ برداریِ داده در فضای ویژگی را در درست داریم و از آن برای محاسبه ضرب‌داخلی استفاده می‌کنیم.

رابطه بین کرنل‌ها و اندازه‌گیری شباهت جالب توجه است: در بین محققان رشته یادگیری ماشین، ضرب‌داخلی به عنوان معیاری از شباهت مورد قبول واقع شده‌است. ولی در نگاه اول این رابطه واضح نیست. در واقع مفهوم شباهت بیشتر با فاصله عجین است. اگر نوعی از فاصله (مثلاً فاصله اقلیدسی) بین دو شیء کم باشد، آنگاه این دو شیء (طبق آن فاصله) شبیه هم در نظر گرفته می‌شوند؛ یعنی مقدار شباهت آن‌ها عددی بزرگ است. گاهی این دو مفهوم همزمان در یک تابع بروز می‌یابند. مثلاً مقدار کرنل RBF (فرمول \ارجا{eq:gaussian-kernel}) معکوسی از فاصله اقلیدسی است و همزمان به دلیل کرنل بودن، حاصل یک ضرب‌داخلی است. در حالت کلی، می‌توان نشان داد که برای هر کرنل $k$ روی هر $\mX$ رابطه
\begin{equation*}
k(x,x^\prime) = \dfrac{\|\phi(x)\|^2 + \|\phi(x^\prime)\|^2 - \|\phi(x) - \phi(x^\prime)\|^2}{2}
\end{equation*}
که در آن $\|z\| = \langle{z,z}\rangle$، برقرار است. همانطور که در فصل یک از \جستار{Scholkopf_2004} توضیح داده شده‌است، $k(x,x^\prime)$ مقدار شباهت بین $x$ و $x^\prime$ را در قالب عکس مجذور فاصله بین $\phi(x)$ و $\phi(x^\prime)$ در فضای ویژگی، تا جمله 
$\|\phi(x)\|^2$ و
$\|\phi(x^\prime)\|^2$
 اندازه می‌گیرد. اگر تمام نقاط طول یکسانی در فضای ویژگی داشته باشند (
برای هر $x \in \mX$ مقدار $\|\phi(x)\|^2$ عددی ثابت باشد
)، آنگاه کرنل یک تابع کاهشی از فاصله در فضای ویژگی است.

برای جمع‌بندی این بخش، از آنجا که کرنل‌ها حاصل ضرب‌داخلی هستند، می‌توان به آنها بعنوان مقیاس شباهت نگاه کرد. و از آنجا که برای هر کرنل، یک تابع نگاشت ویژگی وجود دارد، می‌توان به آنها بعنوان نمایش گراف نگاه کرد. برای بعضی کرنل‌ها، تعریف دقیق نحوه نمایش گراف امکان‌پذیر نیست. اما براساس آنچه توضیح داده شد، محاسبه ضرب داخلی آن امکان‌پذیر است. علاوه بر این، یک امتیاز کرنل این است که برای انواع داده قابل تعریف هستند.

\subsubsection{دسته‌بندی دودویی با استفاده از ماشین‌های بردار پشتیبان}
دسته‌بندی دودویی یکی از مسائل بنیادی در یادگیری ماشین است. به صورت غیر رسمی مسئله به این صورت است که: دو دسته کلاس داده شده‌اند، 

\subsection{مبانی گراف کرنل‌}


\subsection{کرنل‌های مبتنی بر مسیر}
\subsection{کرنل‌های مبتنی بر زیرگراف‌های کوچک}
\subsection{کرنل‌های مبتنی بر زیردرخت‌ها}
